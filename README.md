# AI-Powered-Autonomous-Drone-System-A-PADS-

Remember E.D.I.T.H. from Spider-Man: Far From Home, where Peter used the AI to control drones and access advanced technology left by Tony Stark? That inspired me to create my own version of E.D.I.T.H. I’ve fine-tuned a multimodal model (combining LLM and VIT) to control a Raspberry Pi-based drone with internet connectivity. The camera feed from the drone is sent back to the model for analysis. I’m using Ngrok and Flask to maintain the connection, allowing the drone to be manually controlled via Flask endpoints or autonomously by the model—just like in the movie. All the user needs is to provide a command. The drone is built using a Raspberry Pi Zero, MPU-6050, ESC, A2212 1000KV brushless motor, Ublox NEO-6M GPS module, a USB camera for the video feed, and 1500mah lipo battery.

I have fine-tuned a llama 3 7b model with a VIT model for image analysis. 
Assume I have told the LLM (Llama 3 7b) to watch this region. Then, suddenly, a flood happens. The LLM scrapes data about that place from the internet, containing information like politics, environment, civil war, etc., but it determines the flood-related news is crucial. So, it autonomously sends the drone to the location based on coordinates where the event is happening. The drone's camera feed is sent to the ViT (which captions the video), and the captioned text is passed to the LLM to decide the best course of action while hovering. The ViT model also creates a report about what it observed during the mission.
